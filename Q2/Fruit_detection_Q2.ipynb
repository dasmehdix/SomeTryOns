{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Fruit Object Detection**\n### - Bu çalışmada PyTorch kütüphanesi kullanılarak meyve tespiti(fruit detection) eğitimi ve testi yapılmıştır.\n### - Model olarak TorchVision üzerinden Faster-RCNN Object Detector kullanılmıştır.\n\n\n## Veri Özellikleri:\n### - Fruit Images veriseti kullanılmıştır.\n### - Elma, Muz ve portakal olmak üzere üç farklı sınıf bulunmaktadır.\n### - 240 adet eğitim seti görseli ve etiketi, 60 adet test seti görseli ve etiketi içermektedir.\n### - Etiketler \"labelImg\" ile etiketlenmiştir.\n### - Etiketler XML formatındadır. Object detection şeklinde etiketlendiği içinde XML dosyaları objenin resimdeki koordinatlarını içermektedir.","metadata":{}},{"cell_type":"markdown","source":"## Öncelikle ortamda hazır bulunmayan, kullanmamız gereken bazı scriptleri yüklememiz gerekmektedir.","metadata":{}},{"cell_type":"code","source":"!pip install pycocotools --quiet\n!git clone https://github.com/pytorch/vision.git\n!git checkout v0.3.0\n\n!cp vision/references/detection/utils.py ./\n!cp vision/references/detection/transforms.py ./\n!cp vision/references/detection/coco_eval.py ./\n!cp vision/references/detection/engine.py ./\n!cp vision/references/detection/coco_utils.py ./","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Gerekli kütüphaneler çağırılır.","metadata":{}},{"cell_type":"code","source":"import os\nimport random\nimport numpy as np\nimport pandas as pd\n# for ignoring warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport cv2\n\n# XML ETIKET PARSE İÇİN\nfrom xml.etree import ElementTree as et\n\n# GÖRSELLEŞTİRME\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\n# torchvision libraries\nimport torch\nimport torchvision\nfrom torchvision import transforms as torchtrans  \nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n\n# Eğitim için\nfrom engine import train_one_epoch, evaluate\nimport utils\nimport transforms as T\n\n# Veri Çoğaltma\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Veri DosyaYapısı\n### **--\"train_zip\"**\n### -----\"train\"\n### --------\"apple_1.jpg\"\n### --------\"apple_1.xml\"\n### --------\"apple_n.jpg\"\n### --------\"apple_n.xml\"\n### **--\"test_zip\"**\n### -----\"test\"\n### --------\"apple_78.jpg\"\n### --------\"apple_78.xml\"\n### --------\"apple_n.jpg\"\n### --------\"apple_n.xml\"","metadata":{}},{"cell_type":"markdown","source":"## - Öncelikle veri dizinlerini tanımlıyoruz.\n## - PyTorch'da genelde veri okume & ayrıştırma kısmı ve model oluşturma kısmı Pytorch class'larından inherit edilir.\n## - Ben de bu bölümde \"FruitImagesDataset\" class'ını oluşturdum. Class input olarak veri dizini, görsel boyutu ve transform almaktadır. Class çalışma şekli:\n### - Önce görsel okunur. BGR'den RGB'ye çevilir. Float cinsine çevrilir. Görsel istenen boyutlara indirgenir ve normalize edilir.\n### - XML dosyası ayrıştırılır. Görsele ait Id, obje koordinatları, görsel sınıfı ve alanı hesaplanıp etikette tutulur.","metadata":{}},{"cell_type":"code","source":"# Veriyolları\nfiles_dir = '../input/fruit-images-for-object-detection/train_zip/train'\ntest_dir = '../input/fruit-images-for-object-detection/test_zip/test'\n\n\nclass FruitImagesDataset(torch.utils.data.Dataset):\n\n    def __init__(self, files_dir, width, height, transforms=None):\n        self.transforms = transforms\n        self.files_dir = files_dir\n        self.height = height\n        self.width = width\n        \n        # JPG görüntülerini sıralama\n        self.imgs = [image for image in sorted(os.listdir(files_dir))\n                        if image[-4:]=='.jpg']\n        \n        \n        # Sınıf isimleri(ekstra olarak FasterRCNN kullanıdğım için background koydum)\n        self.classes = [_, 'apple','banana','orange']\n    \n    def __len__(self):\n        return len(self.imgs)\n        \n    def __getitem__(self, idx):\n\n        img_name = self.imgs[idx]\n        image_path = os.path.join(self.files_dir, img_name)\n\n        # Görsel okuma vs    \n        img = cv2.imread(image_path)\n        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32)\n        img_res = cv2.resize(img_rgb, (self.width, self.height), cv2.INTER_AREA)\n        # Normalizasyon önemli\n        img_res /= 255.0\n        \n        # anEtiket dosyası\n        annot_filename = img_name[:-4] + '.xml'\n        annot_file_path = os.path.join(self.files_dir, annot_filename)\n        \n        boxes = []\n        labels = []\n        tree = et.parse(annot_file_path)\n        root = tree.getroot()\n        \n        # cgörsel boyutları\n        wt = img.shape[1]\n        ht = img.shape[0]\n        \n        # bbox koordinatları\n        for member in root.findall('object'):\n            labels.append(self.classes.index(member.find('name').text))\n            \n            # bounding box\n            xmin = int(member.find('bndbox').find('xmin').text)\n            xmax = int(member.find('bndbox').find('xmax').text)\n            \n            ymin = int(member.find('bndbox').find('ymin').text)\n            ymax = int(member.find('bndbox').find('ymax').text)\n            \n            \n            xmin_corr = (xmin/wt)*self.width\n            xmax_corr = (xmax/wt)*self.width\n            ymin_corr = (ymin/ht)*self.height\n            ymax_corr = (ymax/ht)*self.height\n            \n            boxes.append([xmin_corr, ymin_corr, xmax_corr, ymax_corr])\n        \n        # Koordinatları Torch Tensor yapısına çevrilir\n        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n        \n        # Bbox alanı\n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n\n        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)\n        \n        labels = torch.as_tensor(labels, dtype=torch.int64)\n\n\n        target = {}\n        target[\"boxes\"] = boxes\n        target[\"labels\"] = labels\n        target[\"area\"] = area\n        target[\"iscrowd\"] = iscrowd\n        # Görsel Id\n        image_id = torch.tensor([idx])\n        target[\"image_id\"] = image_id\n\n\n        if self.transforms:\n            \n            sample = self.transforms(image = img_res,\n                                     bboxes = target['boxes'],\n                                     labels = labels)\n            \n            img_res = sample['image']\n            target['boxes'] = torch.Tensor(sample['bboxes'])\n            \n            \n            \n        return img_res, target\n\n\n# Veriyi okuyalım bakalım\ndataset = FruitImagesDataset(files_dir, 224, 224)\nprint('length of dataset = ', len(dataset), '\\n')\n\n# Rastgele bir örnek alalım\nimg, target = dataset[78]\nprint(img.shape, '\\n',target)","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Küçük bir görselleştirme..","metadata":{}},{"cell_type":"code","source":"def plot_img_bbox(img, target):\n    fig, a = plt.subplots(1,1)\n    fig.set_size_inches(5,5)\n    a.imshow(img)\n    ## Birden fazla box obje bulunabilir.\n    for box in (target['boxes']):\n        x, y, width, height  = box[0], box[1], box[2]-box[0], box[3]-box[1]\n        rect = patches.Rectangle((x, y),\n                                 width, height,\n                                 linewidth = 2,\n                                 edgecolor = 'r',\n                                 facecolor = 'none')\n\n        a.add_patch(rect)\n    plt.show()\n    \n# Rastgele bir görsele ve etiketine bakalım\nimg, target = dataset[25]\nplot_img_bbox(img, target)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model - FasterRCNN\n## - Popüler klasik modellerden bir tanesidir.\n## - Faster R-CNN iki aşamada incelenebilir: \n### **Region Proposal Network (RPN):** İlk aşama olan RPN bölge önermeye yarayan derin, evrişimli bir sinir ağıdır. RPN, girdi olarak herhangi bir boyutta girdiyi alır ve obje skoruna göre bir dizi nesnelere ait olabilecek dikdörtgen teklifi ortaya çıkarır. Bu öneriyi, evrişimli katman tarafından oluşturulan öznitelik haritası üzerinde küçük bir ağı kaydırarak yapar.\n### **Fast R-CNN:** RPN tarafından üretilen bu hesaplamalar Fast R-CNN mimarisine sokulur ve bir sınıflandırıcı ile objenin sınıfı, regressor ile de bounding box’u tahmin edilir.\n![MİMARİ YAPI](https://d9v7j6n3.rocketcdn.me/wp-content/uploads/2020/12/faster-r-cnn.jpg.webp)\n\n## [PyTorch FasterRCNN](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html) [Fast RCNN](https://teknoloji.org/nesne-tanima-algoritmalari-r-cnn-fast-r-cnn-ve-faster-r-cnn-nedir/)","metadata":{}},{"cell_type":"code","source":"\ndef get_object_detection_model(num_classes):\n\n    # Daha önce eğitilmiş bir fasterrcnn modeli çekelim.\n    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n    \n    # Input Features\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    # sınıf sayımız farklı olabileceğinden dolayı katmanları değiştiriyoruz\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes) \n\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Veri Çoğaltma\n# - Verisetinde toplamda 300 adet görsel bulunmaktadır. Aslında bu rakam çok fazla değildir. Data Augmentation veri sayısını arttırmak için kullanılır.\n# - Bu sefer yapmadım fakat bazen veriye bulanıklık, parlaklık gibi gürültü eklemek modelimizin daha gürbüz(robust) çalışmasını sağlamaktadır.","metadata":{}},{"cell_type":"code","source":"def get_transform(train):\n    \n    if train:\n        return A.Compose([\n                            A.HorizontalFlip(0.5),\n                     # Dikey Flip işlemi\n                            ToTensorV2(p=1.0) \n                        ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n    else:\n        return A.Compose([\n                            ToTensorV2(p=1.0)\n                        ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preparing dataset","metadata":{}},{"cell_type":"code","source":"# Eğitim ve Test Verisini Okuyalım\ndataset = FruitImagesDataset(files_dir, 480, 480, transforms= get_transform(train=True))\ndataset_test = FruitImagesDataset(files_dir, 480, 480, transforms= get_transform(train=False))\n# SEED\ntorch.manual_seed(1)\nindices = torch.randperm(len(dataset)).tolist()\n\n# Veriyi Sub-Train-Val-Test olarak ayrıştıralım\ntest_split = 0.2\ntsize = int(len(dataset)*test_split)\ndataset = torch.utils.data.Subset(dataset, indices[:-tsize])\ndataset_test = torch.utils.data.Subset(dataset_test, indices[-tsize:])\n\n# Veri DataLoader formatına çevirilir.\ndata_loader = torch.utils.data.DataLoader(\n    dataset, batch_size=10, shuffle=True, num_workers=4,\n    collate_fn=utils.collate_fn)\n\ndata_loader_test = torch.utils.data.DataLoader(\n    dataset_test, batch_size=10, shuffle=False, num_workers=4,\n    collate_fn=utils.collate_fn)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Eğitim Ayarları\n## - Öncelikle GPU cihazı seçilir.\n## - Model çağırılır\n## - Optimizer ve hiperparametre tanımlaması yapılır\n## - İsteğe göre callbacks tanımalamaları yapılır. Ben bu denemede sadece \"Learning Rate\" düşürücü kullandım.","metadata":{}},{"cell_type":"code","source":"# GPU SEÇME KISMI\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\n# SINIF SAYISI[ELMA,PORTAKAL,MUZ,ARKAPLAN]\n# FasterRCNN Segmentation temelli bir yapı olduğundan \"arkaplan\" sınıfını ekliyoruz\nnum_classes = 4\n\n# Model Çağırılır\nmodel = get_object_detection_model(num_classes)\n\n# Model GPU üzerine oturtulur\nmodel.to(device)\n\n# Optimizer ve Hiperparametre Tanımlaması.SGD seçilmiştir.\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(params, lr=0.005,\n                            momentum=0.9, weight_decay=0.0005)\n\n# Learning Rate Düşürücü\nlr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n                                               step_size=3,\n                                               gamma=0.1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Eğitim\n## - 10 Epochluk bir eğitim tasarlanmıştır.\n## - Hiperparamtereler, loss değerleri ve AP değerleri gözlenmektedir. \n## - Eğitim sonucunda yaklaşık test seti için 0.78-0.8 AP değerine ulaşılmıştır.","metadata":{}},{"cell_type":"code","source":"# training for 10 epochs\nnum_epochs = 10\n\nfor epoch in range(num_epochs):\n    # training for one epoch\n    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n    # update the learning rate\n    lr_scheduler.step()\n    # evaluate on the test dataset\n    evaluate(model, data_loader_test, device=device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modelimizi Test Edelim\n## - Bu aşamada test setimizden rastgele bir örnek alıp çıktısını alalım.\n## - Görmüş olduğunuz gibi model çıktısı 14 BBox gösterirken olması gereken BBox sayısı 1.\n## - Bu farkın sebebi FasterRCNN NonMax Supression yapmamasıdır. \"apply_nms\" fonksiyonu model çıktısına NonMaxSupression yaparak en doğru bbox'u bulmaktadır.","metadata":{}},{"cell_type":"code","source":"def apply_nms(orig_prediction, iou_thresh=0.3):\n    \n    # NonMax Supression Processing\n    keep = torchvision.ops.nms(orig_prediction['boxes'], orig_prediction['scores'], iou_thresh)\n    \n    final_prediction = orig_prediction\n    final_prediction['boxes'] = final_prediction['boxes'][keep]\n    final_prediction['scores'] = final_prediction['scores'][keep]\n    final_prediction['labels'] = final_prediction['labels'][keep]\n    \n    return final_prediction\n\n# Torch2PILLOW\ndef torch_to_pil(img):\n    return torchtrans.ToPILImage()(img).convert('RGB')\n\n# rastgele bir veri seçelim\nimg, target = dataset_test[5]\n# put the model in evaluation mode\nmodel.eval()\nwith torch.no_grad():\n    prediction = model([img.to(device)])[0]\n    \nprint('predicted #boxes: ', len(prediction['labels']))\nprint('real #boxes: ', len(target['labels']))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## - Etiket Görselleştirmesi(Tek BBOX)","metadata":{}},{"cell_type":"code","source":"print('EXPECTED OUTPUT')\nplot_img_bbox(torch_to_pil(img), target)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## - Model Çıktısı (14 BBOX)","metadata":{}},{"cell_type":"code","source":"print('MODEL OUTPUT')\nplot_img_bbox(torch_to_pil(img), prediction)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## - Model çıktısını NonMaxSupression işleminden geçirince istediğimiz sonuca ulaştığını gözlemliyoruz.\n## - Modelimizin tek dilimi de algıladığını gözlemliyoruz. Bu aslında istemediğimiz durumdur.\n## - Bu durumdan kurtulmak için modelimizin başarımını arttırabilir, NonMaxSupression işlemiyle alakalı küçük değişikler yapabilir, veriye gürültü ekleyebiliriz.","metadata":{}},{"cell_type":"code","source":"nms_prediction = apply_nms(prediction, iou_thresh=0.2)\nprint('NMS APPLIED MODEL OUTPUT')\nplot_img_bbox(torch_to_pil(img), nms_prediction)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tüm test işlemleri tek bir kod parçacığında gösterelim. \n### - Rastgele bir görsel alıp, modelimizin nasıl sonuç verdiğine bakalım.\n### - Modelimizin çıktısını NMS işleminden geçirelim, etiktele beraber görselleştirelim.","metadata":{}},{"cell_type":"code","source":"test_dataset = FruitImagesDataset(test_dir, 480, 480, transforms= get_transform(train=True))\n# Rastgele Görsel ve Etiket\nimg, target = test_dataset[10]\n# EVAL MOD\nmodel.eval()\nwith torch.no_grad():\n    prediction = model([img.to(device)])[0]\n    \nprint('EXPECTED OUTPUT\\n')\nplot_img_bbox(torch_to_pil(img), target)\nprint('MODEL OUTPUT\\n')\nnms_prediction = apply_nms(prediction, iou_thresh=0.01)\n\nplot_img_bbox(torch_to_pil(img), nms_prediction)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}